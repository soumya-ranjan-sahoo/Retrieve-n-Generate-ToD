{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512cb2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ssahoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ssahoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import os \n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "#from num2words import num2words\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c458db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def convert_lower_case(data):\n",
    "    return data.lower()\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    data = re.sub(r'[\"!\\\"#$%&()*+-./:;<=>?@[\\]^`{|}~\\n\"]',' ',data)\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    #data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    #print(data)\n",
    "    data = stemming(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def CreateCollection(KBNumber,json_data):\n",
    "        \"\"\"\n",
    "        Creates collection for the IR Task\n",
    "        Collection here is the set of KBs associated with a specific conversation\n",
    "        Each document in the collection is representative of indivual KB. \n",
    "        Parameters :\n",
    "        KBNumber - Number of the KG ; Used for indexing a specific KB\n",
    "        json_data - The input file that has the KBs in it\n",
    "    \n",
    "        \"\"\" \n",
    "        try:\n",
    "          \n",
    "            KB = json_data[KBNumber][\"kg\"]\n",
    "            collectionList =[]\n",
    "            for record in KB:\n",
    "                dictList=[]\n",
    "                for key, value in record.items():\n",
    "                    #print(\"key-\",key,\"Value-\",value)\n",
    "                    dictList.append(key+\":\"+value)\n",
    "                collectionList.append(dictList)\n",
    "            return collectionList  \n",
    "        except Exception as e:\n",
    "            print(\"Oops!\", e.__class__, \"occurred.\")\n",
    "            return 0\n",
    "        \n",
    "\n",
    "def CreateNewCollection(dialog):\n",
    "        \"\"\"\n",
    "        %Currently in use --- V2.0%\n",
    "        Creates collection for the IR Task\n",
    "        Collection here is the set of KBs associated with a specific conversation\n",
    "        Each document in the collection is representative of indivual KB. \n",
    "        Parameters :\n",
    "        KBNumber - Number of the KG ; Used for indexing a specific KB\n",
    "        json_data - The input file that has the KBs in it\n",
    "    \n",
    "        \"\"\" \n",
    "        try:\n",
    "          \n",
    "            KB = dialog[\"kg\"]\n",
    "            collectionList =[]\n",
    "            for record in KB:\n",
    "                dictList=[]\n",
    "                for key, value in record.items():\n",
    "                    #print(\"key-\",key,\"Value-\",value)\n",
    "                    dictList.append(key+\":\"+value)\n",
    "                collectionList.append(dictList)\n",
    "            return collectionList  \n",
    "        except Exception as e:\n",
    "            print(\"Oops!\", e.__class__, \"occurred.\")\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "def CreateConversation(convNumber,json_data):\n",
    "        \"\"\"\n",
    "        Conversation history and gold utterance are created here.\n",
    "        Note : We use the current utterance too as history\n",
    "        Parameters :\n",
    "        convNumber - Number of the Conversation ; should be same as KBNumber in CreateCollection()\n",
    "        json_data - The input file that has the KBs in it\n",
    "    \n",
    "        \"\"\" \n",
    "    \n",
    "        queryConversation = []\n",
    "        goldUtterance = []\n",
    "        try:\n",
    "            for conversation in json_data[convNumber][\"dialogue\"]:\n",
    "                \n",
    "                if (conversation[\"data\"][\"end_dialogue\"]) != True:\n",
    "                    queryConversation.append((conversation[\"data\"][\"utterance\"]).lower())\n",
    "                else:\n",
    "                    goldUtterance.append((conversation[\"data\"][\"utterance\"]).lower())\n",
    "                    break\n",
    "            return queryConversation, goldUtterance\n",
    "        except Exception as e:\n",
    "            print(\"Oops!\", e.__class__, \"occurred.\")\n",
    "            return 0, 0\n",
    "            \n",
    "def SlidingWindow(convNumber, json_data, windowSize):\n",
    "        \"\"\"\n",
    "        Creates a Sliding Window of size windowSize for the IR Task which would be a query\n",
    "        Moving window is created using the conversation history and the current utterance\n",
    "        Parameters :\n",
    "        convNumber - Number of the Conversation ; should be same as KBNumber in CreateCollection()\n",
    "        json_data - The input file that has the KBs in it\n",
    "        windowSize - Moving Window size for the utterance \n",
    "    \n",
    "        \"\"\" \n",
    "        queryConversation, goldUtterance = CreateConversation(convNumber,json_data)\n",
    "        queries = []\n",
    "        for conversation in queryConversation:\n",
    "            #print(conversation)\n",
    "            #conversation = preprocess(conversation)\n",
    "            split_sequence = conversation.lower().split()\n",
    "            iteration_length = len(split_sequence) - (windowSize - 1)\n",
    "            max_window_indicies = range(iteration_length)\n",
    "            for index in max_window_indicies:\n",
    "                queries.append(split_sequence[index:index + windowSize])\n",
    "        return queryConversation, goldUtterance, queries\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "### DOCUMENT FREQUENCY UTILITY ###\n",
    "\n",
    "def CreateDF(collectionList):\n",
    "    DF = {}\n",
    "    for i,kb in enumerate(collectionList):\n",
    "        for record in kb:\n",
    "            record = remove_punctuation(record)\n",
    "            tokens = word_tokenize(remove_punctuation(record))\n",
    "            for w in tokens:\n",
    "                try:\n",
    "                    DF[w].add(i)\n",
    "                except:\n",
    "                    DF[w] = {i}\n",
    "    for i in DF:\n",
    "        DF[i] = len(DF[i])\n",
    "    \n",
    "    return DF\n",
    "\n",
    "    \n",
    "def DocFreq(DF,word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### TF-IDF CALCULATOR ###\n",
    "\n",
    "def CalculateTFIDF(DF,collectionList):\n",
    "    \n",
    "    \n",
    "    docNo = 0\n",
    "    N = len(collectionList)\n",
    "    tf_idf = {}\n",
    "    \n",
    "    for i,kb in enumerate(collectionList):\n",
    "    \n",
    "        for record in kb:\n",
    "            record = remove_punctuation(record)\n",
    "            tokens = word_tokenize(remove_punctuation(record))\n",
    "            counter = Counter(tokens)\n",
    "            words_count = len(tokens)\n",
    "            for token in np.unique(tokens):\n",
    "                tf = counter[token]/words_count\n",
    "                df = DocFreq(DF,token)\n",
    "                idf = np.log((N+1)/(df+1))\n",
    "                tf_idf[docNo, token] = tf\n",
    "        docNo += 1\n",
    "    assert (N == docNo)\n",
    "    return tf_idf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### IDF CALCULATOR ###\n",
    "def CalculateIDF(DF,collectionList):\n",
    "    docNo = 0\n",
    "    N = len(collectionList)\n",
    "    _idf = {}\n",
    "    for i,kb in enumerate(collectionList):\n",
    "    \n",
    "        for record in kb:\n",
    "            record = remove_punctuation(record)\n",
    "            tokens = word_tokenize(remove_punctuation(record))\n",
    "            counter = Counter(tokens)\n",
    "            words_count = len(tokens)\n",
    "            for token in np.unique(tokens):\n",
    "                #tf = counter[token]/words_count\n",
    "                df = DocFreq(DF,token)\n",
    "                idf = np.log((N+1)/(df+1))\n",
    "                _idf[docNo, token] = idf\n",
    "        docNo += 1\n",
    "    assert (N == docNo)\n",
    "    return _idf\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "### AVG-IDF CALCULATOR ###\n",
    "def CalculateAvgIDF(collectionList,query):\n",
    "    queryScore = 0\n",
    "    N = len(collectionList)\n",
    "    for token in query:\n",
    "        df = DocFreq(DF,token)\n",
    "        queryScore+= np.log((N+1)/(df+1))    \n",
    "    return queryScore/3\n",
    "    \n",
    "\n",
    "    \n",
    "def RelevantDocs(query,docMatrix):\n",
    "    \n",
    "    tokens = query\n",
    "    query_weights = {}\n",
    "    for key in docMatrix:\n",
    "        \n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += docMatrix[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = docMatrix[key]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)  \n",
    "    docIds = []\n",
    "    for i in query_weights[:10]:  ### selecting top 10 relevant docs\n",
    "        docIds.append(i[0])\n",
    "    return docIds\n",
    "    \n",
    "\n",
    "def KRelevantRecords(relevantDocDict,numberRelevantDocs):\n",
    "    \n",
    "    from collections import Counter\n",
    "    \n",
    "    flat_list = []\n",
    "    docids = []\n",
    "    try:\n",
    "        flat_list = [item for sublist in list(relevantDocDict.values()) for item in sublist]\n",
    "        data = Counter(flat_list)\n",
    "        for docs in data.most_common(numberRelevantDocs):\n",
    "            docids.append(docs[0])\n",
    "        return docids    \n",
    "        \n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def CreateHistory(n,json_data):\n",
    "    return json_data[n][\"history\"]\n",
    "\n",
    "\n",
    "def CreateNewHistory(dialog):\n",
    "    \"\"\"\n",
    "    %Currently in use --- V2.0%\n",
    "    \"\"\"\n",
    "    return dialog[\"history\"]\n",
    "\n",
    "\n",
    "def precision_at_k(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(k)\n",
    "    return result\n",
    "\n",
    "def recall_at_k(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    try:\n",
    "        result = len(act_set & pred_set) / float(len(act_set))\n",
    "    except:\n",
    "        result = 0.0\n",
    "    return result\n",
    "\n",
    "\n",
    "def AverageScore(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "def AverageF1(precision,recall):\n",
    "    return 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "def DRIVERUTILITY(json_data,numberRelevantDocs):\n",
    "    \"\"\"\n",
    "    Driver Method\n",
    "    \"\"\"\n",
    "    N = len(json_data)\n",
    "    print(\"The json files has %3d KBs\"%(len(json_data)))\n",
    "    relevanceDict = {}\n",
    "    for n in range(6):\n",
    "        collectionList = CreateCollection(n,json_data)\n",
    "        #queryConversation, goldUtterance = CreateConversation(n,json_data)\n",
    "        queryHistory = CreateHistory(n,json_data)\n",
    "        if collectionList == 0 or queryHistory == 0:\n",
    "            relevanceDict[n] = [] # KB/Utterance Not Available\n",
    "        else:\n",
    "            DF = CreateDF(collectionList)\n",
    "            tf_idf = CalculateTFIDF(DF,collectionList)\n",
    "            relevantDocDict = {}\n",
    "            docMatrix = tf_idf\n",
    "            for idx, query in enumerate(queryHistory):\n",
    "                query = query.lower().split()\n",
    "                relevantDocDict[str(query)] = RelevantDocs(numberRelevantDocs,query,docMatrix)\n",
    "            docID = KRelevantRecords(relevantDocDict,numberRelevantDocs)\n",
    "            relevanceDict[n] = docID\n",
    "    return relevanceDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2f4b578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dialogue data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8529/8529 [01:49<00:00, 77.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_precision 0.5670066830812522\n",
      "avg_recall 0.3533761564187066\n",
      "AverageF1 0.4353984749212212\n",
      "emptyListCount 0.19146441552350804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Train and Validate - INCAR and CAMREST for IR Eval###\n",
    "os.chdir(\"C:\\\\WorkDirectory\\\\DialogueSystems\\\\Code-Thesis\\\\BaseModel\\\\kg-structure-aware-dialogues\")\n",
    "\n",
    "def match_kgentry(kg, gold):\n",
    "        \"\"\"\n",
    "        kg: all the knowledge in the form of dictionary as it is for instance the kg in val.json\n",
    "        gold: reference entity list\n",
    "        return: ranked/weighted list of relevant documents based on entity counts. \n",
    "        \"\"\"\n",
    "        count = list()\n",
    "        print(kg)\n",
    "        print(gold)\n",
    "        for i, entry in enumerate(kg):\n",
    "            current_count = 0\n",
    "            for ent in gold:\n",
    "                if ent in list(entry.values()):\n",
    "                    current_count += 1\n",
    "            count.append(current_count)\n",
    "        return count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _prepare_conversations(dataset, split_type):\n",
    "        print(\"Loading dialogue data...\")\n",
    "        formatted_dialogs = json.load(open(join(\"data\",dataset,split_type+\".json\")))\n",
    "        return formatted_dialogs\n",
    "\n",
    "dialogs = _prepare_conversations(dataset=\"woz2_1\", split_type=\"train\") ### REPLACE NAME OF THE DATASET AND THE SET\n",
    "numberRelevantDocs = 2\n",
    "precision_at_k_list = []\n",
    "recall_at_k_list = []\n",
    "emptyListCount = 0\n",
    "\n",
    "for dialog in tqdm(dialogs):\n",
    "        ref_entities = dialog[\"ref_ents\"]\n",
    "        actual = []\n",
    "        predicted = []\n",
    "        kg_list = []\n",
    "        if dialog[\"kg\"]:\n",
    "            for i,kg in enumerate(dialog[\"kg\"]):\n",
    "                kg_list.append((' '.join(kg.values())))\n",
    "                if all(ent in kg.values() for ent in ref_entities):\n",
    "                    actual.append(i)                \n",
    "            relevanceDict = {}\n",
    "            collectionList = CreateNewCollection(dialog)\n",
    "            queryHistory = CreateNewHistory(dialog)\n",
    "            queryHistory = [queryHistory[-1]] if len(queryHistory) == 1 else queryHistory[-2:] ### last 2 dialogues\n",
    "            if collectionList == 0 or queryHistory == 0:\n",
    "                relevanceDict[dialog[\"id\"]] = []  # KB/Utterance Not Available\n",
    "            else:\n",
    "                DF = CreateDF(collectionList)\n",
    "                tf_idf = CalculateTFIDF(DF,collectionList)\n",
    "                relevantDocDict = {}\n",
    "                docMatrix = tf_idf\n",
    "                for idx, query in enumerate(queryHistory):\n",
    "                    query = query.lower().split()\n",
    "                   \n",
    "                    query = [word for word in query if word not in stopwords.words('english')]\n",
    "        \n",
    "                    relevantDocDict[str(query)] = RelevantDocs(query,docMatrix)\n",
    "                docID = KRelevantRecords(relevantDocDict,numberRelevantDocs)\n",
    "                relevanceDict[dialog[\"id\"]] = docID\n",
    "            Dict = relevanceDict \n",
    "            predicted = [item for sublist in [*Dict.values()] for item in sublist] \n",
    "            for key, vals in Dict.items():\n",
    "                kgitem = \" \"\n",
    "                for val in vals:\n",
    "                    knowledge = dialog['kg'][val]\n",
    "                    for k,v in knowledge.items():\n",
    "                        kgitem+=\" \"+k+\" \"+v\n",
    "            used_knowledge = kgitem\n",
    "            if not actual or not predicted:\n",
    "                emptyListCount+=1   \n",
    "            precision_at_k_list.append(precision_at_k(actual,predicted,numberRelevantDocs))\n",
    "            recall_at_k_list.append(recall_at_k(actual,predicted,numberRelevantDocs))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "avg_precision = AverageScore(precision_at_k_list)\n",
    "avg_recall = AverageScore(recall_at_k_list)\n",
    "print(\"avg_precision\",avg_precision) \n",
    "print(\"avg_recall\",avg_recall) \n",
    "print(\"AverageF1\",AverageF1(avg_precision,avg_recall))\n",
    "print(\"emptyListCount\",emptyListCount/len(dialogs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf77582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
