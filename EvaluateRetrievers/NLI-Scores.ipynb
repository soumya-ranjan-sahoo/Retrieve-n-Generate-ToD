{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d76988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ssahoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ssahoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dialogue data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/8529 [00:00<?, ?it/s]The `multi_class` argument has been deprecated and renamed to `multi_label`. `multi_class` will be removed in a future version of Transformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In class method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                               | 1/8529 [00:02<5:07:51,  2.17s/it]The `multi_class` argument has been deprecated and renamed to `multi_label`. `multi_class` will be removed in a future version of Transformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In class method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `multi_class` argument has been deprecated and renamed to `multi_label`. `multi_class` will be removed in a future version of Transformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In class method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 1/8529 [00:04<11:36:38,  4.90s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4344\\2913522861.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueryHistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m                     \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkg_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m                     \u001b[0mkg_relevant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnumberRelevantDocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;31m#print(len(kg_relevant))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4344\\2913522861.py\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self, sequence, candidate_labels, multi_class)\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"In class method\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmulti_class\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBARTClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs_use_gpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Anaconda3\\envs\\kgconv\\lib\\site-packages\\transformers\\pipelines\\zero_shot_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, sequences, candidate_labels, hypothesis_template, multi_label, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m             \u001b[0msequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhypothesis_template\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[1;31m# XXX: Some tokenizers cannot handle batching because they don't\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Anaconda3\\envs\\kgconv\\lib\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_and_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    764\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Anaconda3\\envs\\kgconv\\lib\\site-packages\\transformers\\pipelines\\zero_shot_classification.py\u001b[0m in \u001b[0;36m_forward\u001b[1;34m(self, inputs, return_tensors)\u001b[0m\n\u001b[0;32m    156\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m                         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_tensor_on_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m                         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Anaconda3\\envs\\kgconv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Anaconda3\\envs\\kgconv\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1446\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1447\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1448\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1449\u001b[0m         )\n\u001b[0;32m   1450\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# last hidden state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Anaconda3\\envs\\kgconv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Anaconda3\\envs\\kgconv\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1171\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1173\u001b[1;33m                 \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1174\u001b[0m             )\n\u001b[0;32m   1175\u001b[0m         \u001b[1;31m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Anaconda3\\envs\\kgconv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Anaconda3\\envs\\kgconv\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    802\u001b[0m                         \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m                         \u001b[0mlayer_head_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 804\u001b[1;33m                         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    805\u001b[0m                     )\n\u001b[0;32m    806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Anaconda3\\envs\\kgconv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Anaconda3\\envs\\kgconv\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_dropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Anaconda3\\envs\\kgconv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Anaconda3\\envs\\kgconv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Anaconda3\\envs\\kgconv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import pipeline\n",
    "import os\n",
    "from os.path import join\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "os.chdir(\"path-to-your-directory\") ### path of your directory \n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import os \n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "#from num2words import num2words\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def convert_lower_case(data):\n",
    "    return data.lower()\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    data = re.sub(r'[\"!\\\"#$%&()*+-./:;<=>?@[\\]^`{|}~\\n\"]',' ',data)\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    #data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data) \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def CreateCollection(KBNumber,json_data):\n",
    "        \"\"\"\n",
    "        Creates collection for the IR Task\n",
    "        Collection here is the set of KBs associated with a specific conversation\n",
    "        Each document in the collection is representative of indivual KB. \n",
    "        Parameters :\n",
    "        KBNumber - Number of the KG ; Used for indexing a specific KB\n",
    "        json_data - The input file that has the KBs in it\n",
    "    \n",
    "        \"\"\" \n",
    "        try:\n",
    "          \n",
    "            KB = json_data[KBNumber][\"kg\"]\n",
    "            collectionList =[]\n",
    "            for record in KB:\n",
    "                dictList=[]\n",
    "                for key, value in record.items():\n",
    "                    #print(\"key-\",key,\"Value-\",value)\n",
    "                    dictList.append(key+\":\"+value)\n",
    "                collectionList.append(dictList)\n",
    "            return collectionList  \n",
    "        except Exception as e:\n",
    "            print(\"Oops!\", e.__class__, \"occurred.\")\n",
    "            return 0\n",
    "        \n",
    "\n",
    "def CreateNewCollection(dialog):\n",
    "        \"\"\"\n",
    "        %Currently in use --- V2.0%\n",
    "        Creates collection for the IR Task\n",
    "        Collection here is the set of KBs associated with a specific conversation\n",
    "        Each document in the collection is representative of indivual KB. \n",
    "        Parameters :\n",
    "        KBNumber - Number of the KG ; Used for indexing a specific KB\n",
    "        json_data - The input file that has the KBs in it\n",
    "    \n",
    "        \"\"\" \n",
    "        try:\n",
    "          \n",
    "            KB = dialog[\"kg\"]\n",
    "            collectionList =[]\n",
    "            for record in KB:\n",
    "                dictList=[]\n",
    "                for key, value in record.items():\n",
    "                    #print(\"key-\",key,\"Value-\",value)\n",
    "                    dictList.append(key+\":\"+value)\n",
    "                collectionList.append(dictList)\n",
    "            return collectionList  \n",
    "        except Exception as e:\n",
    "            print(\"Oops!\", e.__class__, \"occurred.\")\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "def CreateConversation(convNumber,json_data):\n",
    "        \"\"\"\n",
    "        Conversation history and gold utterance are created here.\n",
    "        Note : We use the current utterance too as history\n",
    "        Parameters :\n",
    "        convNumber - Number of the Conversation ; should be same as KBNumber in CreateCollection()\n",
    "        json_data - The input file that has the KBs in it\n",
    "    \n",
    "        \"\"\" \n",
    "    \n",
    "        queryConversation = []\n",
    "        goldUtterance = []\n",
    "        try:\n",
    "            for conversation in json_data[convNumber][\"dialogue\"]:\n",
    "                \n",
    "                if (conversation[\"data\"][\"end_dialogue\"]) != True:\n",
    "                    queryConversation.append((conversation[\"data\"][\"utterance\"]).lower())\n",
    "                else:\n",
    "                    goldUtterance.append((conversation[\"data\"][\"utterance\"]).lower())\n",
    "                    break\n",
    "            return queryConversation, goldUtterance\n",
    "        except Exception as e:\n",
    "            print(\"Oops!\", e.__class__, \"occurred.\")\n",
    "            return 0, 0\n",
    "            \n",
    "def SlidingWindow(convNumber, json_data, windowSize):\n",
    "        \"\"\"\n",
    "        Creates a Sliding Window of size windowSize for the IR Task which would be a query\n",
    "        Moving window is created using the conversation history and the current utterance\n",
    "        Parameters :\n",
    "        convNumber - Number of the Conversation ; should be same as KBNumber in CreateCollection()\n",
    "        json_data - The input file that has the KBs in it\n",
    "        windowSize - Moving Window size for the utterance \n",
    "    \n",
    "        \"\"\" \n",
    "        queryConversation, goldUtterance = CreateConversation(convNumber,json_data)\n",
    "        queries = []\n",
    "        for conversation in queryConversation:\n",
    "            #print(conversation)\n",
    "            #conversation = preprocess(conversation)\n",
    "            split_sequence = conversation.lower().split()\n",
    "            iteration_length = len(split_sequence) - (windowSize - 1)\n",
    "            max_window_indicies = range(iteration_length)\n",
    "            for index in max_window_indicies:\n",
    "                queries.append(split_sequence[index:index + windowSize])\n",
    "        return queryConversation, goldUtterance, queries\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "### DOCUMENT FREQUENCY UTILITY ###\n",
    "\n",
    "def CreateDF(collectionList):\n",
    "    DF = {}\n",
    "    for i,kb in enumerate(collectionList):\n",
    "        for record in kb:\n",
    "            record = remove_punctuation(record)\n",
    "            tokens = word_tokenize(remove_punctuation(record))\n",
    "            for w in tokens:\n",
    "                try:\n",
    "                    DF[w].add(i)\n",
    "                except:\n",
    "                    DF[w] = {i}\n",
    "    for i in DF:\n",
    "        DF[i] = len(DF[i])\n",
    "    \n",
    "    return DF\n",
    "\n",
    "    \n",
    "def DocFreq(DF,word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### TF-IDF CALCULATOR ###\n",
    "\n",
    "def CalculateTFIDF(DF,collectionList):\n",
    "    \n",
    "    \n",
    "    docNo = 0\n",
    "    N = len(collectionList)\n",
    "    tf_idf = {}\n",
    "    \n",
    "    for i,kb in enumerate(collectionList):\n",
    "    \n",
    "        for record in kb:\n",
    "            record = remove_punctuation(record)\n",
    "            tokens = word_tokenize(remove_punctuation(record))\n",
    "            counter = Counter(tokens)\n",
    "            words_count = len(tokens)\n",
    "            for token in np.unique(tokens):\n",
    "                tf = counter[token]/words_count\n",
    "                df = DocFreq(DF,token)\n",
    "                idf = np.log((N+1)/(df+1))\n",
    "                tf_idf[docNo, token] = tf\n",
    "        docNo += 1\n",
    "    assert (N == docNo)\n",
    "    return tf_idf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### IDF CALCULATOR ###\n",
    "def CalculateIDF(DF,collectionList):\n",
    "    docNo = 0\n",
    "    N = len(collectionList)\n",
    "    _idf = {}\n",
    "    for i,kb in enumerate(collectionList):\n",
    "    \n",
    "        for record in kb:\n",
    "            record = remove_punctuation(record)\n",
    "            tokens = word_tokenize(remove_punctuation(record))\n",
    "            counter = Counter(tokens)\n",
    "            words_count = len(tokens)\n",
    "            for token in np.unique(tokens):\n",
    "                #tf = counter[token]/words_count\n",
    "                df = DocFreq(DF,token)\n",
    "                idf = np.log((N+1)/(df+1))\n",
    "                _idf[docNo, token] = idf\n",
    "        docNo += 1\n",
    "    assert (N == docNo)\n",
    "    return _idf\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "### AVG-IDF CALCULATOR ###\n",
    "def CalculateAvgIDF(collectionList,query):\n",
    "    queryScore = 0\n",
    "    N = len(collectionList)\n",
    "    for token in query:\n",
    "        df = DocFreq(DF,token)\n",
    "        queryScore+= np.log((N+1)/(df+1))    \n",
    "    return queryScore/3\n",
    "    \n",
    "\n",
    "    \n",
    "def RelevantDocs(query,docMatrix):\n",
    "    \n",
    "    tokens = query\n",
    "    query_weights = {}\n",
    "    for key in docMatrix:\n",
    "        \n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += docMatrix[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = docMatrix[key]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)  \n",
    "    docIds = []\n",
    "    for i in query_weights[:10]:  ### selecting top 10 relevant docs\n",
    "        docIds.append(i[0])\n",
    "    return docIds\n",
    "    \n",
    "\n",
    "def KRelevantRecords(relevantDocDict,numberRelevantDocs):\n",
    "    \n",
    "    from collections import Counter\n",
    "    \n",
    "    flat_list = []\n",
    "    docids = []\n",
    "    try:\n",
    "        flat_list = [item for sublist in list(relevantDocDict.values()) for item in sublist]\n",
    "        data = Counter(flat_list)\n",
    "        for docs in data.most_common(numberRelevantDocs):\n",
    "            docids.append(docs[0])\n",
    "        return docids    \n",
    "        \n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def CreateHistory(n,json_data):\n",
    "    return json_data[n][\"history\"]\n",
    "\n",
    "\n",
    "def CreateNewHistory(dialog):\n",
    "    \"\"\"\n",
    "    %Currently in use --- V2.0%\n",
    "    \"\"\"\n",
    "    return dialog[\"history\"]\n",
    "\n",
    "\n",
    "def precision_at_k(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(k)\n",
    "    return result\n",
    "\n",
    "def recall_at_k(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    try:\n",
    "        result = len(act_set & pred_set) / float(len(act_set))\n",
    "    except:\n",
    "        result = 0.0\n",
    "    return result\n",
    "\n",
    "\n",
    "def AverageScore(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "def AverageF1(precision,recall):\n",
    "    return 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "def DRIVERUTILITY(json_data,numberRelevantDocs):\n",
    "    \"\"\"\n",
    "    Driver Method\n",
    "    \"\"\"\n",
    "    N = len(json_data)\n",
    "    print(\"The json files has %3d KBs\"%(len(json_data)))\n",
    "    relevanceDict = {}\n",
    "    for n in range(6):\n",
    "        collectionList = CreateCollection(n,json_data)\n",
    "        #queryConversation, goldUtterance = CreateConversation(n,json_data)\n",
    "        queryHistory = CreateHistory(n,json_data)\n",
    "        if collectionList == 0 or queryHistory == 0:\n",
    "            relevanceDict[n] = [] # KB/Utterance Not Available\n",
    "        else:\n",
    "            DF = CreateDF(collectionList)\n",
    "            tf_idf = CalculateTFIDF(DF,collectionList)\n",
    "            relevantDocDict = {}\n",
    "            docMatrix = tf_idf\n",
    "            for idx, query in enumerate(queryHistory):\n",
    "                query = query.lower().split()\n",
    "                relevantDocDict[str(query)] = RelevantDocs(numberRelevantDocs,query,docMatrix)\n",
    "            docID = KRelevantRecords(relevantDocDict,numberRelevantDocs)\n",
    "            relevanceDict[n] = docID\n",
    "    return relevanceDict\n",
    "\n",
    "\n",
    "\n",
    "def match_kgentry(kg, gold):\n",
    "        \"\"\"\n",
    "        kg: all the knowledge in the form of dictionary as it is for instance the kg in val.json\n",
    "        gold: reference entity list\n",
    "        \"\"\"\n",
    "        count = list()\n",
    "        print(kg)\n",
    "        print(gold)\n",
    "        for i, entry in enumerate(kg):\n",
    "            current_count = 0\n",
    "            for ent in gold:\n",
    "                if ent in list(entry.values()):\n",
    "                    current_count += 1\n",
    "            count.append(current_count)\n",
    "        return count\n",
    "\n",
    "class BARTClassifier:\n",
    "    def __init__(self, args_use_gpu):\n",
    "        self.classifier = pipeline(\"zero-shot-classification\", model='facebook/bart-large-mnli', device=0 if args_use_gpu else -1)\n",
    "\n",
    "    def classify(self, sequence, candidate_labels, multi_class=True):\n",
    "        print(\"In class method\")\n",
    "        return self.classifier(sequence, candidate_labels) if multi_class is None else self.classifier(sequence, candidate_labels, multi_class=True)\n",
    "\n",
    "classifier = BARTClassifier(args_use_gpu = False)\n",
    "    \n",
    "    \n",
    "def _prepare_conversations(dataset=\"woz2_1\", split_type=\"train\"):\n",
    "        print(\"Loading dialogue data...\")\n",
    "        formatted_dialogs = json.load(open(join(\"data\",dataset,split_type+\".json\")))\n",
    "        return formatted_dialogs\n",
    "\n",
    "dialogs = _prepare_conversations(dataset=\"woz2_1\", split_type=\"train\")\n",
    "dialogs = dialogs\n",
    "numberRelevantDocs = 2  ### RETRIVED DOCS\n",
    "precision_at_k_list = []\n",
    "recall_at_k_list = []\n",
    "emptyListCount = 0\n",
    "classifier = BARTClassifier(args_use_gpu = False)\n",
    "for dialog in tqdm(dialogs):\n",
    "        ref_entities = dialog[\"ref_ents\"]\n",
    "        actual = []\n",
    "        predicted = []\n",
    "        kg_list = []\n",
    "        if dialog[\"kg\"]:\n",
    "            for i,kg in enumerate(dialog[\"kg\"]):\n",
    "                kg_list.append((' '.join(kg.values())))\n",
    "                if all(ent in kg.values() for ent in ref_entities):\n",
    "                    actual.append(i)    \n",
    "            queryHistory = CreateNewHistory(dialog)\n",
    "            #queryHistory = [queryHistory[-1]] if len(queryHistory) == 1 else queryHistory[-2:] ### QUERY SIZE - last 2 dialogues\n",
    "            if queryHistory == 0:\n",
    "                predicted = [] # KB/Utterance Not Available\n",
    "            else:\n",
    "                for idx, query in enumerate(queryHistory):\n",
    "                    ans = classifier.classify(query,kg_list)\n",
    "                    kg_relevant = ans[\"labels\"][:numberRelevantDocs]\n",
    "                #print(len(kg_relevant))\n",
    "                for kg in kg_relevant:\n",
    "                        predicted.append(kg_list.index(kg))\n",
    "            if not actual or not predicted:\n",
    "                emptyListCount+=1\n",
    "            precision_at_k_list.append(precision_at_k(actual,predicted,numberRelevantDocs))\n",
    "            recall_at_k_list.append(recall_at_k(actual,predicted,numberRelevantDocs))\n",
    "\n",
    "\n",
    "print(\"------------------------------------------------------------------\")\n",
    "avg_precision = AverageScore(precision_at_k_list)\n",
    "avg_recall = AverageScore(recall_at_k_list)\n",
    "print(\"avg_precision\",avg_precision) \n",
    "print(\"avg_recall\",avg_recall) \n",
    "print(\"AverageF1\",AverageF1(avg_precision,avg_recall))\n",
    "print(\"emptyListCount\",emptyListCount/len(dialogs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702747cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
